{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csc_matrix as csc\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    ")\n",
    "import time\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from constants import (SEED, EPSILON, EVENT_THRESHOLD, DEFAULT_K, DEFAULT_THRESHOLD,  UX_CONSTANTS, LOG_DIR, \n",
    "                       DATA_DIR, TEST_DATA_PATH, DATA_OCT, DATA_NOV, USECOLS, USER, ITEM, RATING, PREDICTION)\n",
    "from utilities.ms_evaluation import (rmse, auc, logloss, precision_at_k, recall_at_k, ndcg_at_k, map_at_k, mae, rsquared, exp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = r'BasicMatrixFactorization'\n",
    "Y_HAT_PATH = DATA_DIR+r'/'+NAME+r'-y_hat.npz'\n",
    "TEST_RESULTS_PATH = LOG_DIR+'\\\\'+NAME+'\\\\test-results.csv'\n",
    "SKL = \"(calculated using sklearn.metrics on non-zero values of sparse matrices)\"\n",
    "SPA = \"(calculated using CSC sparse matrix operations)\"\n",
    "MSE = \"(calculated using the Microsoft Evaluation method)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the matrices: (177592, 44780)\n",
      "Number of non-zero values:\n",
      "Y:  552,255\n",
      "Ŷ:  552,255\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "log = pd.Series(dtype='float64')\n",
    "y_hat = sp.load_npz(Y_HAT_PATH) \n",
    "y = sp.load_npz(TEST_DATA_PATH)\n",
    "assert y_hat.shape == y.shape, 'The shape of Y and Y_hat must match, otherwise they are not comparable.'\n",
    "print(f\"Shape of the matrices: {y.shape}\")\n",
    "print(\"Number of non-zero values:\")\n",
    "print(f\"Y: {y.nnz:8,}\")\n",
    "print(f\"Ŷ: {y_hat.nnz:8,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_dense(matrix):\n",
    "    matrix = matrix.todok()\n",
    "    dense = []\n",
    "    rows,cols = matrix.nonzero()\n",
    "    for row,col in zip(rows,cols):\n",
    "        dense.append([row,col,matrix[row,col]])\n",
    "    return dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.DataFrame(data=sparse_to_dense(y),columns=[USER, ITEM, RATING])\n",
    "df_pred = pd.DataFrame(data=sparse_to_dense(y_hat),columns=[USER, ITEM, PREDICTION])\n",
    "# Usually, the CSC is used when there are more rows than columns. (If there are more columns, use CSR instead.)\n",
    "y_hat = y_hat.tocsc()\n",
    "y = y.tocsc()\n",
    "y_nz = np.array(y[y.nonzero()]).reshape(-1)\n",
    "y_hat_nz = np.array(y_hat[y_hat.nonzero()]).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 1.310757711564404 (calculated using CSC sparse matrix operations)\n",
      "Mean Square Error: 1.3107577562332153 (calculated using sklearn.metrics on non-zero values of sparse matrices)\n",
      "Note: The smaller the better.\n"
     ]
    }
   ],
   "source": [
    "mse_spa = csc.sum(csc.power(y_hat-y,2))/y.nnz\n",
    "mse_skl = mean_squared_error(y_nz,y_hat_nz)\n",
    "print(f\"Mean Square Error: {mse_spa} {SPA}\")\n",
    "print(f\"Mean Square Error: {mse_skl} {SKL}\")\n",
    "print('Note: The smaller the better.')\n",
    "log[\"mse\"]=mse_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error: 1.144883274209386 (calculated using CSC sparse matrix operations)\n",
      "Root Mean Square Error: 1.1448832750320435 (calculated using sklearn.metrics on non-zero values of sparse matrices)\n",
      "Root Mean Square Error: 1.1448832537005298 (calculated using the Microsoft Evaluation method)\n",
      "Note: The smaller the better.\n"
     ]
    }
   ],
   "source": [
    "rmse_spa = np.sqrt(mse_spa)\n",
    "rmse_skl = np.sqrt(mse_skl)\n",
    "rmse_mse = rmse(df_true,df_pred)\n",
    "print(f\"Root Mean Square Error: {rmse_spa} {SPA}\")\n",
    "print(f\"Root Mean Square Error: {rmse_skl} {SKL}\")\n",
    "print(f\"Root Mean Square Error: {rmse_mse} {MSE}\")\n",
    "print('Note: The smaller the better.')\n",
    "log[\"rmse\"]=rmse_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.5893737381282198 (calculated using CSC sparse matrix operations)\n",
      "Mean Absolute Error: 0.589373767375946 (calculated using sklearn.metrics on non-zero values of sparse matrices)\n",
      "Mean Absolute Error: 0.5893737007193129 (calculated using the Microsoft Evaluation method)\n",
      "Note: The smaller the better.\n"
     ]
    }
   ],
   "source": [
    "mae_spa = csc.sum(abs(y_hat-y))/y.nnz\n",
    "mae_skl = mean_absolute_error(y_nz,y_hat_nz)\n",
    "mae_mse = mae(df_true,df_pred)\n",
    "print(f\"Mean Absolute Error: {mae_spa} {SPA}\")\n",
    "print(f\"Mean Absolute Error: {mae_skl} {SKL}\")\n",
    "print(f\"Mean Absolute Error: {mae_mse} {MSE}\")\n",
    "print('Note: The smaller the better.')\n",
    "log[\"mae\"]=mae_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (R²): -17.556097329188866 (calculated using sklearn.metrics on non-zero values of sparse matrices)\n",
      "Coefficient of determination (R²): -17.556096339367596 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "r2_skl = r2_score(y_nz,y_hat_nz)\n",
    "r2_mse = rsquared(df_true,df_pred)\n",
    "print(f\"Coefficient of determination (R\\u00B2): {r2_skl} {SKL}\")\n",
    "print(f\"Coefficient of determination (R\\u00B2): {r2_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[\"r-squared\"]=r2_skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: -16.636564254760742\n",
      "Explained variance: -16.6365574884413 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "exp_var_skl = explained_variance_score(y_nz,y_hat_nz)\n",
    "exp_var_mse = exp_var(df_true,df_pred)\n",
    "print(f\"Explained variance: {exp_var_skl}\")\n",
    "print(f\"Explained variance: {exp_var_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[\"exp_var\"]=exp_var_skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arear Under Curve (AUC) - integral area under the receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For AUC and logloss we need binary ratings and predictions in the [0,1] range\n",
    "df_true_bin = df_true.copy()\n",
    "ux_constants = pd.Series(pd.read_csv(UX_CONSTANTS, index_col=0, squeeze=True, header=None), dtype='float32')\n",
    "df_true_bin[RATING] = df_true_bin[RATING].apply(lambda x: 1 if x > ux_constants['positive_above'] else 0)\n",
    "df_pred_bin = df_pred.copy()\n",
    "df_pred_bin[PREDICTION] = minmax_scale(df_pred_bin[PREDICTION].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arear Under Curve (AUC): 0.5017609006803636 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better. 0.5 indicates an uninformative classifier\n"
     ]
    }
   ],
   "source": [
    "auc_mse = auc(df_true_bin,df_pred_bin)\n",
    "print(f\"Arear Under Curve (AUC): {auc_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better. 0.5 indicates an uninformative classifier\")\n",
    "log[\"auc\"]=auc_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic loss (logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss (logloss): 0.8076549730069438 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 0 the better.\n"
     ]
    }
   ],
   "source": [
    "logloss_mse = logloss(df_true_bin,df_pred_bin)\n",
    "print(f\"Logistic loss (logloss): {logloss_mse} {MSE}\")\n",
    "print(\"Note: The closer to 0 the better.\")\n",
    "log[\"logloss\"]=logloss_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision @ K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @ 10: 0.34191752705560396 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "precision_at_k_mse = precision_at_k(df_true,df_pred)\n",
    "print(f\"Precision @ {DEFAULT_K}: {precision_at_k_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"precision-at-{DEFAULT_K}\"]=precision_at_k_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall @ K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 10: 0.9668412602660089 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "recall_at_k_mse = recall_at_k(df_true,df_pred)\n",
    "print(f\"Recall @ {DEFAULT_K}: {recall_at_k_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"recall-at-{DEFAULT_K}\"]=recall_at_k_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalized Discounted Cumulative Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized Discounted Cumulative Gain (nDCG@10): 1.0 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "ndcg_mse = ndcg_at_k(df_true,df_pred)\n",
    "print(f\"normalized Discounted Cumulative Gain (nDCG@{DEFAULT_K}): {ndcg_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"ndcg-at-{DEFAULT_K}\"]=ndcg_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mAP (mean Average Precision) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Average Precision (mAP@10): 0.9668412602660089 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "map_mse = map_at_k(df_true,df_pred)\n",
    "print(f\"mean Average Precision (mAP@{DEFAULT_K}): {map_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"map-at-{DEFAULT_K}\"]=map_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse                 1.310758\n",
       "rmse                1.144883\n",
       "mae                 0.589374\n",
       "r-squared         -17.556097\n",
       "exp_var           -16.636564\n",
       "auc                 0.501761\n",
       "logloss             0.807655\n",
       "precision-at-10     0.341918\n",
       "recall-at-10        0.966841\n",
       "ndcg-at-10          1.000000\n",
       "map-at-10           0.966841\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 181.56 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Elapsed time: {time.time()-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.to_csv(TEST_RESULTS_PATH, index = True, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
