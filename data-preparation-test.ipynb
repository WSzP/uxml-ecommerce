{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this notebook is to test the data preparation against the ground truth.**\n",
    "\n",
    "This notebook is not needed for predicting user behaviours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csc_matrix as csc\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    ")\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from constants import (SEED, EPSILON, EVENT_THRESHOLD, DEFAULT_K, DEFAULT_THRESHOLD, LOG_DIR, UX_CONSTANTS,\n",
    "                       DATA_DIR, TEST_DATA_PATH, DATA_OCT, DATA_NOV, USECOLS, USER, ITEM, RATING, PREDICTION)\n",
    "from utilities.ms_evaluation import (rmse, auc, logloss, precision_at_k, recall_at_k, ndcg_at_k, map_at_k, mae, rsquared, exp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RESULTS_PATH = DATA_DIR+'\\\\data-preparation-test-results.csv'\n",
    "MSE = \"(calculated using the Microsoft Evaluation method)\"\n",
    "ux_constants = pd.Series(pd.read_csv(UX_CONSTANTS, index_col=0, squeeze=True, header=None), dtype='float32')\n",
    "POSITIVE_ABOVE = ux_constants['positive_above']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-zero values:  552,255\n"
     ]
    }
   ],
   "source": [
    "### NOTE: the y_hat from this test will serve as y in testing the machine learning models\n",
    "### For our purposes (accessing elements of the sparse matrix) DOK is the fastest format\n",
    "log = pd.Series(dtype='float64')\n",
    "y_hat = sp.load_npz(TEST_DATA_PATH) # use this to test the data preparation, \n",
    "y_hat = y_hat.todok()\n",
    "print(f\"Number of non-zero values: {y_hat.nnz:8,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data preparation y against ground truth purchases events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of purchase events in test the dataset: 547912\n"
     ]
    }
   ],
   "source": [
    "input_df = pd.concat([pd.read_csv(DATA_OCT, engine='c', sep=',',usecols=USECOLS)\n",
    "                ,pd.read_csv(DATA_NOV, engine='c', sep=',',usecols=USECOLS)])\n",
    "drop_visitors = set(input_df.user_id.value_counts()[input_df.user_id.value_counts()<EVENT_THRESHOLD].index)\n",
    "input_df = input_df[~input_df.user_id.isin(drop_visitors)]\n",
    "input_df.reset_index(inplace=True,drop=True)\n",
    "new_user_id = pd.Series(pd.read_csv(DATA_DIR+r'new_user_id.csv', index_col=1, squeeze=True), dtype='int32')\n",
    "new_product_id = pd.Series(pd.read_csv(DATA_DIR+r'new_product_id.csv', index_col=1, squeeze=True), dtype='int32')\n",
    "input_df = input_df[input_df.event_type=='purchase']\n",
    "input_df = input_df.drop(columns=['event_type'])\n",
    "purchases = set()\n",
    "\n",
    "for row in input_df.itertuples(): \n",
    "    uid = new_user_id[row.user_id]\n",
    "    pid = new_product_id[row.product_id]\n",
    "    purchases.add((uid,pid))    \n",
    "print(f\"Number of purchase events in test the dataset: {len(purchases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating\n",
       "0       0       0       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true = []\n",
    "rows,cols = y_hat.nonzero()\n",
    "for row,col in zip(rows,cols):\n",
    "    if (row,col) in purchases:\n",
    "        df_true.append([row,col,1])\n",
    "    else:\n",
    "        df_true.append([row,col,0])\n",
    "df_true = pd.DataFrame(data=df_true,columns=[USER, ITEM, RATING])\n",
    "df_true.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  prediction\n",
       "0       0       0           1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepared = []\n",
    "rows,cols = y_hat.nonzero()\n",
    "for row,col in zip(rows,cols):\n",
    "    if y_hat[row,col] > POSITIVE_ABOVE:\n",
    "        df_prepared.append([row,col,1])\n",
    "    else:\n",
    "        df_prepared.append([row,col,0])  \n",
    "#     df_prepared.append([row,col,y_hat[row,col]])\n",
    "df_prepared = pd.DataFrame(data=df_prepared,columns=[USER, ITEM, PREDICTION])\n",
    "df_prepared.head(1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error: 0.06022414545897269 (calculated using the Microsoft Evaluation method)\n",
      "Note: The smaller the better.\n"
     ]
    }
   ],
   "source": [
    "rmse_mse = rmse(df_true,df_prepared)\n",
    "print(f\"Root Mean Square Error: {rmse_mse} {MSE}\")\n",
    "print('Note: The smaller the better.')\n",
    "log[\"rmse\"]=rmse_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.0036269476962635014 (calculated using the Microsoft Evaluation method)\n",
      "Note: The smaller the better.\n"
     ]
    }
   ],
   "source": [
    "mae_mse = mae(df_true,df_prepared)\n",
    "print(f\"Mean Absolute Error: {mae_mse} {MSE}\")\n",
    "print('Note: The smaller the better.')\n",
    "log[\"mae\"]=mae_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (R²): 0.9713705377724504 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "r2_mse = rsquared(df_true,df_prepared)\n",
    "print(f\"Coefficient of determination (R\\u00B2): {r2_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[\"r-squared\"]=r2_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.9713896353776997 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "exp_var_mse = exp_var(df_true,df_prepared)\n",
    "print(f\"Explained variance: {exp_var_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[\"exp_var\"]=exp_var_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arear Under Curve (AUC) - integral area under the receiver operating characteristic curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arear Under Curve (AUC): 0.994998401692711 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better. 0.5 indicates an uninformative classifier\n"
     ]
    }
   ],
   "source": [
    "auc_mse = auc(df_true,df_prepared)\n",
    "print(f\"Arear Under Curve (AUC): {auc_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better. 0.5 indicates an uninformative classifier\")\n",
    "log[\"auc\"]=auc_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic loss (logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss (logloss): 0.12527240738957413 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 0 the better.\n"
     ]
    }
   ],
   "source": [
    "logloss_mse = logloss(df_true,df_prepared)\n",
    "print(f\"Logistic loss (logloss): {logloss_mse} {MSE}\")\n",
    "print(\"Note: The closer to 0 the better.\")\n",
    "log[\"logloss\"]=logloss_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision @ K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @ 10: 0.34191752705560396 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "precision_at_k_mse = precision_at_k(df_true,df_prepared)\n",
    "print(f\"Precision @ {DEFAULT_K}: {precision_at_k_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"precision-at-{DEFAULT_K}\"]=precision_at_k_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall @ K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 10: 0.9668412602660089 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "recall_at_k_mse = recall_at_k(df_true,df_prepared)\n",
    "print(f\"Recall @ {DEFAULT_K}: {recall_at_k_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"recall-at-{DEFAULT_K}\"]=recall_at_k_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalized Discounted Cumulative Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized Discounted Cumulative Gain (nDCG@10): 1.0 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "ndcg_mse = ndcg_at_k(df_true,df_prepared)\n",
    "print(f\"normalized Discounted Cumulative Gain (nDCG@{DEFAULT_K}): {ndcg_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"ndcg-at-{DEFAULT_K}\"]=ndcg_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mAP (mean Average Precision) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Average Precision (mAP@10): 0.9668412602660089 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "map_mse = map_at_k(df_true,df_prepared)\n",
    "print(f\"mean Average Precision (mAP@{DEFAULT_K}): {map_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"map-at-{DEFAULT_K}\"]=map_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rmse              0.060224\n",
       "mae               0.003627\n",
       "r-squared         0.971371\n",
       "exp_var           0.971390\n",
       "auc               0.994998\n",
       "logloss           0.125272\n",
       "precision-at-10   0.341918\n",
       "recall-at-10      0.966841\n",
       "ndcg-at-10        1.000000\n",
       "map-at-10         0.966841\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.to_csv(TEST_RESULTS_PATH, index = True, header=False)\n",
    "log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
