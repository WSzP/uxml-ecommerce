{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse import csc_matrix as csc\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    explained_variance_score,\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    ")\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from constants import (SEED, EPSILON, EVENT_THRESHOLD, DEFAULT_K, DEFAULT_THRESHOLD, LOG_DIR, \n",
    "                       DATA_DIR, TEST_DATA_PATH, DATA_OCT, DATA_NOV, USECOLS, USER, ITEM, RATING, PREDICTION)\n",
    "from utilities.ms_evaluation import (rmse, auc, logloss, precision_at_k, recall_at_k, ndcg_at_k, map_at_k, mae, rsquared, exp_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = r'BasicMatrixFactorization' \n",
    "Y_HAT_PATH = DATA_DIR+r'/'+NAME+r'-y_hat.npz'\n",
    "TEST_RESULTS_PATH = LOG_DIR+'\\\\'+NAME+'\\\\test-results.csv'\n",
    "SKL = \"(calculated using sklearn.metrics on non-zero values of sparse matrices)\"\n",
    "SPA = \"(calculated using CSC sparse matrix operations)\"\n",
    "MSE = \"(calculated using the Microsoft Evaluation method)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the matrices: (177592, 44780)\n",
      "Number of non-zero values:\n",
      "Y:  552,255\n",
      "Ŷ:  552,255\n"
     ]
    }
   ],
   "source": [
    "log = pd.Series(dtype='float64')\n",
    "# y_hat = sp.load_npz(Y_HAT_PATH) \n",
    "y_hat = sp.load_npz(TEST_DATA_PATH) # use this to test the data preparation\n",
    "\n",
    "y = sp.load_npz(TEST_DATA_PATH)\n",
    "assert y_hat.shape == y.shape, 'The shape of Y and Y_hat must match, otherwise they are not comparable.'\n",
    "print(f\"Shape of the matrices: {y.shape}\")\n",
    "print(\"Number of non-zero values:\")\n",
    "print(f\"Y: {y.nnz:8,}\")\n",
    "print(f\"Ŷ: {y_hat.nnz:8,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually, the CSC is used when there are more rows than columns. (If there are more columns, use CSR instead.)\n",
    "y_hat = y_hat.tocsc()\n",
    "y = y.tocsc()\n",
    "y_nz = np.array(y[y.nonzero()]).reshape(-1)\n",
    "y_hat_nz = np.array(y_hat[y_hat.nonzero()]).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test run for the recommender engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of purchase events in test the dataset: 547912\n"
     ]
    }
   ],
   "source": [
    "input_df = pd.concat([pd.read_csv(DATA_OCT, engine='c', sep=',',usecols=USECOLS)\n",
    "                ,pd.read_csv(DATA_NOV, engine='c', sep=',',usecols=USECOLS)])\n",
    "drop_visitors = set(input_df.user_id.value_counts()[input_df.user_id.value_counts()<EVENT_THRESHOLD].index)\n",
    "input_df = input_df[~input_df.user_id.isin(drop_visitors)]\n",
    "input_df.reset_index(inplace=True,drop=True)\n",
    "new_user_id = pd.Series(pd.read_csv(DATA_DIR+r'new_user_id.csv', index_col=1, squeeze=True), dtype='int32')\n",
    "new_product_id = pd.Series(pd.read_csv(DATA_DIR+r'new_product_id.csv', index_col=1, squeeze=True), dtype='int32')\n",
    "input_df = input_df[input_df.event_type=='purchase']\n",
    "input_df = input_df.drop(columns=['event_type'])\n",
    "purchases = set()\n",
    "\n",
    "for row in input_df.itertuples(): \n",
    "    uid = new_user_id[row.user_id]\n",
    "    pid = new_product_id[row.product_id]\n",
    "    purchases.add((uid,pid))    \n",
    "print(f\"Number of purchase events in test the dataset: {len(purchases)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating\n",
       "0       0       0       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true = []\n",
    "ydok = y.todok()\n",
    "rows,cols = ydok.nonzero()\n",
    "for row,col in zip(rows,cols):\n",
    "    if (row,col) in purchases:\n",
    "        df_true.append([row,col,1])\n",
    "    else:\n",
    "        df_true.append([row,col,0])\n",
    "df_true = pd.DataFrame(data=df_true,columns=[USER, ITEM, RATING])\n",
    "df_true.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.831493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  prediction\n",
       "0       0       0    0.831493"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = []\n",
    "y_hat_dok = y_hat.todok()\n",
    "rows,cols = y_hat_dok.nonzero()\n",
    "for row,col in zip(rows,cols):\n",
    "    df_pred.append([row,col,y_hat_dok[row,col]])\n",
    "df_pred = pd.DataFrame(data=df_pred,columns=[USER, ITEM, PREDICTION])\n",
    "df_pred.head(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.858894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  prediction\n",
       "0       0       0    0.858894"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We must convert the predicted ratings into a [0, 1] scale for auc and logloss metrics\n",
    "df_pred_bin = df_pred.copy()\n",
    "df_pred_bin[PREDICTION] = minmax_scale(df_pred_bin[PREDICTION].astype(float))\n",
    "df_pred_bin.head(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.0 (calculated using CSC sparse matrix operations)\n",
      "Mean Square Error: 0.0 (calculated using sklearn.metrics on non-zero values of sparse matrices)\n",
      "Note: The smaller the better.\n"
     ]
    }
   ],
   "source": [
    "mse_spa = csc.sum(csc.power(y_hat-y,2))/y.nnz\n",
    "mse_skl = mean_squared_error(y_nz,y_hat_nz)\n",
    "print(f\"Mean Square Error: {mse_spa} {SPA}\")\n",
    "print(f\"Mean Square Error: {mse_skl} {SKL}\")\n",
    "print('Note: The smaller the better.')\n",
    "log[\"mse\"]=mse_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error: 0.0 (calculated using CSC sparse matrix operations)\n",
      "Root Mean Square Error: 0.0 (calculated using sklearn.metrics on non-zero values of sparse matrices)\n",
      "Root Mean Square Error: 0.19223674526303544 (calculated using the Microsoft Evaluation method)\n",
      "Note: The smaller the better.\n"
     ]
    }
   ],
   "source": [
    "rmse_spa = np.sqrt(mse_spa)\n",
    "rmse_skl = np.sqrt(mse_skl)\n",
    "rmse_mse = rmse(df_true,df_pred)\n",
    "print(f\"Root Mean Square Error: {rmse_spa} {SPA}\")\n",
    "print(f\"Root Mean Square Error: {rmse_skl} {SKL}\")\n",
    "print(f\"Root Mean Square Error: {rmse_mse} {MSE}\")\n",
    "print('Note: The smaller the better.')\n",
    "log[\"rmse\"]=rmse_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.0 (calculated using CSC sparse matrix operations)\n",
      "Mean Absolute Error: 0.0 (calculated using sklearn.metrics on non-zero values of sparse matrices)\n",
      "Mean Absolute Error: 0.15693475282212682 (calculated using the Microsoft Evaluation method)\n",
      "Note: The smaller the better.\n"
     ]
    }
   ],
   "source": [
    "mae_spa = csc.sum(abs(y_hat-y))/y.nnz\n",
    "mae_skl = mean_absolute_error(y_nz,y_hat_nz)\n",
    "mae_mse = mae(df_true,df_pred)\n",
    "print(f\"Mean Absolute Error: {mae_spa} {SPA}\")\n",
    "print(f\"Mean Absolute Error: {mae_skl} {SKL}\")\n",
    "print(f\"Mean Absolute Error: {mae_mse} {MSE}\")\n",
    "print('Note: The smaller the better.')\n",
    "log[\"mae\"]=mae_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination (R²): 1.0 (calculated using sklearn.metrics on non-zero values of sparse matrices)\n",
      "Coefficient of determination (R²): 0.7082944397370843 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "r2_skl = r2_score(y_nz,y_hat_nz)\n",
    "r2_mse = rsquared(df_true,df_pred)\n",
    "print(f\"Coefficient of determination (R\\u00B2): {r2_skl} {SKL}\")\n",
    "print(f\"Coefficient of determination (R\\u00B2): {r2_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[\"r-squared\"]=r2_skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 1.0\n",
      "Explained variance: 0.7994728055627172 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "exp_var_skl = explained_variance_score(y_nz,y_hat_nz)\n",
    "exp_var_mse = exp_var(df_true,df_pred)\n",
    "print(f\"Explained variance: {exp_var_skl}\")\n",
    "print(f\"Explained variance: {exp_var_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[\"exp_var\"]=exp_var_skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arear Under Curve (AUC) - integral area under the receiver operating characteristic curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arear Under Curve (AUC): 0.9999187163872164 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better. 0.5 indicates an uninformative classifier\n"
     ]
    }
   ],
   "source": [
    "auc_mse = auc(df_true,df_pred_bin)\n",
    "print(f\"Arear Under Curve (AUC): {auc_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better. 0.5 indicates an uninformative classifier\")\n",
    "log[\"auc\"]=auc_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic loss (logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss (logloss): 0.14514958198559486 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 0 the better.\n"
     ]
    }
   ],
   "source": [
    "logloss_mse = logloss(df_true,df_pred_bin)\n",
    "print(f\"Logistic loss (logloss): {logloss_mse} {MSE}\")\n",
    "print(\"Note: The closer to 0 the better.\")\n",
    "log[\"logloss\"]=logloss_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision @ K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision @ 10: 0.34191752705560396 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "precision_at_k_mse = precision_at_k(df_true,df_pred)\n",
    "print(f\"Precision @ {DEFAULT_K}: {precision_at_k_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"precision-at-{DEFAULT_K}\"]=precision_at_k_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall @ K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall @ 10: 0.9668412602660089 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "recall_at_k_mse = recall_at_k(df_true,df_pred)\n",
    "print(f\"Recall @ {DEFAULT_K}: {recall_at_k_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"recall-at-{DEFAULT_K}\"]=recall_at_k_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalized Discounted Cumulative Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized Discounted Cumulative Gain (nDCG@10): 1.0 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "ndcg_mse = ndcg_at_k(df_true,df_pred)\n",
    "print(f\"normalized Discounted Cumulative Gain (nDCG@{DEFAULT_K}): {ndcg_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"ndcg-at-{DEFAULT_K}\"]=ndcg_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mAP (mean Average Precision) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean Average Precision (mAP@10): 0.9668412602660089 (calculated using the Microsoft Evaluation method)\n",
      "Note: The closer to 1 the better.\n"
     ]
    }
   ],
   "source": [
    "map_mse = map_at_k(df_true,df_pred)\n",
    "print(f\"mean Average Precision (mAP@{DEFAULT_K}): {map_mse} {MSE}\")\n",
    "print(\"Note: The closer to 1 the better.\")\n",
    "log[f\"map-at-{DEFAULT_K}\"]=map_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse               0.000000\n",
       "rmse              0.000000\n",
       "mae               0.000000\n",
       "r-squared         1.000000\n",
       "exp_var           1.000000\n",
       "auc               0.999919\n",
       "logloss           0.145150\n",
       "precision-at-10   0.341918\n",
       "recall-at-10      0.966841\n",
       "ndcg-at-10        1.000000\n",
       "map-at-10         0.966841\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.to_csv(TEST_RESULTS_PATH, index = True, header=False)\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
